{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3042667",
   "metadata": {
    "deletable": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2db97f",
   "metadata": {
    "deletable": false,
    "test": "grade"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4e85e70",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The directory `data` contains a subdirectory named `docbin`, which contains two spaCy *DocBin* objects named `small.spacy` and `medium.spacy`.\n",
    "\n",
    "Both *DocBin* objects contain the same texts, but which have been processed using different language models. \n",
    "\n",
    "Whereas the `small.spacy` was created using the small language model for English (`en_core_web_sm`), the file `medium.spacy` was created using the medium language model (`en_core_web_md`).\n",
    "\n",
    "Load the *Doc* objects contained in `small.spacy` and store them into a list under the variable `small_docs`.\n",
    "\n",
    "Then load the *Doc* objects in `medium.spacy` and store them into a list under the variable `medium_docs`.\n",
    "\n",
    "Run the cell below to download the *DocBin* objects on your server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5635b119",
   "metadata": {
    "deletable": false,
    "test": "load_docs"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# docbin_loaded = DocBin().from_disk(path='data/docbin.spacy')\n",
    "small_nlp = spacy.load('en_core_web_sm')\n",
    "medium_nlp = spacy.load('en_core_web_md')\n",
    "small_docs = list(DocBin().from_disk(path='data/docbin/small.spacy').get_docs(small_nlp.vocab))\n",
    "medium_docs = list(DocBin().from_disk(path='data/docbin/medium.spacy').get_docs(medium_nlp.vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7dbd2",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Collect fine-grained part-of-speech tags for all *Doc* objects in both `small_docs` and `medium_docs`. Store the part-of-speech tags into lists named `small_tokens` and `medium_tokens`, respectively.\n",
    "\n",
    "Next, calculate the *precision* score between `medium_tokens` and `small_tokens` to assess to what extent the models produce similar predictions for part-of-speech tags.\n",
    "\n",
    "To do so, use the `precision_score()` function from the `metrics` module of the scikit-learn library. Use `micro` averaging and set the `zero_division` argument to 0.\n",
    "\n",
    "Store the result under the variable `pr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d932507",
   "metadata": {
    "deletable": false,
    "test": "calc_pr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9613049388309923"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "small_tokens = []\n",
    "medium_tokens = []\n",
    "for s in small_docs:\n",
    "    for token in s:\n",
    "        small_tokens.append(token.tag_)\n",
    "for m in medium_docs:\n",
    "    for token in m:\n",
    "        medium_tokens.append(token.tag_)\n",
    "        \n",
    "pr = metrics.precision_score(medium_tokens, small_tokens, average=\"micro\", zero_division=0)\n",
    "pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb8b38",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The directory `data` contains a subdirectory named `corp`, which contains a plain text file with comma-separated values named `pos.csv`.\n",
    "\n",
    "Read the file contents using pandas and store it into a DataFrame under the variable `data`.\n",
    "\n",
    "Get the five most common part-of-speech tags from the column `pos` and store this information into a pandas *Series* named `top_pos`.\n",
    "\n",
    "Then get the five most common tokens for all tokens tagged as `NOUN`, and store this information as a pandas *Series* under the variable `top_nouns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443e1177",
   "metadata": {
    "deletable": false,
    "test": "pandas"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/corp/pos.csv')\n",
    "n = 5\n",
    "top_pos = data['pos'].value_counts()[:n].sort_values(ascending=False)\n",
    "\n",
    "#above_35 = titanic[titanic[\"Age\"] > 35] (Dataframe subset)\n",
    "sub = data[data['pos'] == 'NOUN']\n",
    "top_nouns = sub.value_counts()[:n].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d6f67f-0b56-4cbb-b02b-bec84fc208c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token  pos \n",
       "pct    NOUN    57\n",
       "year   NOUN    42\n",
       "dlrs   NOUN    36\n",
       "oil    NOUN    35\n",
       "mln    NOUN    30\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2273a9d-1bce-46b6-9373-f139c8d7553f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
